{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f82d5ac2-48c0-4aed-afec-ead39d5732bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T16:24:44.599899Z",
     "iopub.status.busy": "2024-11-24T16:24:44.599305Z",
     "iopub.status.idle": "2024-11-24T16:24:52.037265Z",
     "shell.execute_reply": "2024-11-24T16:24:52.036634Z",
     "shell.execute_reply.started": "2024-11-24T16:24:44.599840Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets import CIFAR10, CIFAR100, MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77888212-bed1-428f-804c-a60c0e5cc8cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T16:24:52.054036Z",
     "iopub.status.busy": "2024-11-24T16:24:52.052801Z",
     "iopub.status.idle": "2024-11-24T16:24:52.058237Z",
     "shell.execute_reply": "2024-11-24T16:24:52.057714Z",
     "shell.execute_reply.started": "2024-11-24T16:24:52.054010Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IndexedDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.dataset[idx]\n",
    "        return data, label, idx  # 데이터, 라벨, 인덱스 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3053aa34-e05c-4ce9-a2bb-c62161d08fb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T16:24:52.061255Z",
     "iopub.status.busy": "2024-11-24T16:24:52.060208Z",
     "iopub.status.idle": "2024-11-24T16:24:52.068514Z",
     "shell.execute_reply": "2024-11-24T16:24:52.067844Z",
     "shell.execute_reply.started": "2024-11-24T16:24:52.061230Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 데이터 로더 함수\n",
    "def get_dataloaders(dataset_name, noise_type, noise_rate, batch_size=128):\n",
    "    if dataset_name == \"cifar10\":\n",
    "        train_dataset = CIFAR10(\n",
    "            root=\"./data\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=ToTensor(),\n",
    "        )\n",
    "        test_dataset = CIFAR10(\n",
    "            root=\"./data\",\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=ToTensor(),\n",
    "        )\n",
    "        input_channel = 3\n",
    "        num_classes = 10\n",
    "\n",
    "    elif dataset_name == \"cifar100\":\n",
    "        train_dataset = CIFAR100(\n",
    "            root=\"./data\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=ToTensor(),\n",
    "        )\n",
    "        test_dataset = CIFAR100(\n",
    "            root=\"./data\",\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=ToTensor(),\n",
    "        )\n",
    "        input_channel = 3\n",
    "        num_classes = 100\n",
    "\n",
    "    elif dataset_name == \"mnist\":\n",
    "        train_dataset = MNIST(\n",
    "            root=\"./data\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=ToTensor(),\n",
    "        )\n",
    "        test_dataset = MNIST(\n",
    "            root=\"./data\",\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=ToTensor(),\n",
    "        )\n",
    "        input_channel = 1\n",
    "        num_classes = 10\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset. Choose from: cifar10, cifar100, mnist.\")\n",
    "\n",
    "    train_dataset = IndexedDataset(train_dataset)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    return train_loader, test_loader, input_channel, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e97aabcf-9206-426b-b1ca-ac3be1ce0249",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T16:24:52.071599Z",
     "iopub.status.busy": "2024-11-24T16:24:52.070627Z",
     "iopub.status.idle": "2024-11-24T16:24:54.570512Z",
     "shell.execute_reply": "2024-11-24T16:24:54.569973Z",
     "shell.execute_reply.started": "2024-11-24T16:24:52.071574Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset: cifar10\n",
      "Input Channel: 3, Number of Classes: 10\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 선택\n",
    "dataset_name = \"cifar10\"  # 'cifar10', 'cifar100', 'mnist' 중 선택\n",
    "noise_type = \"symmetric\"  # 노이즈 유형: 'symmetric', 'pairflip'\n",
    "noise_rate = 0.2          # 노이즈 비율\n",
    "batch_size = 128\n",
    "\n",
    "# 데이터 로더 가져오기\n",
    "train_loader, test_loader, input_channel, num_classes = get_dataloaders(\n",
    "    dataset_name, noise_type, noise_rate, batch_size\n",
    ")\n",
    "\n",
    "# 데이터셋 정보 출력\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(f\"Input Channel: {input_channel}, Number of Classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31c8c408-e65e-41d4-8f86-9aacf25607d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T16:24:54.574331Z",
     "iopub.status.busy": "2024-11-24T16:24:54.573349Z",
     "iopub.status.idle": "2024-11-24T16:24:54.580544Z",
     "shell.execute_reply": "2024-11-24T16:24:54.579982Z",
     "shell.execute_reply.started": "2024-11-24T16:24:54.574306Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_coteaching(y_1, y_2, t, forget_rate, ind, noise_or_not):\n",
    "    loss_1 = F.cross_entropy(y_1, t, reduction='none')  # 'reduce' 대신 'reduction' 사용\n",
    "    loss_1_numpy = loss_1.detach().cpu().numpy()  # GPU 텐서를 NumPy 배열로 변환\n",
    "    ind_1_sorted = np.argsort(loss_1_numpy)  # NumPy의 argsort 사용\n",
    "    loss_1_sorted = loss_1[ind_1_sorted]\n",
    "\n",
    "    loss_2 = F.cross_entropy(y_2, t, reduction='none')\n",
    "    loss_2_numpy = loss_2.detach().cpu().numpy()\n",
    "    ind_2_sorted = np.argsort(loss_2_numpy)\n",
    "    loss_2_sorted = loss_2[ind_2_sorted]\n",
    "\n",
    "    remember_rate = 1 - forget_rate\n",
    "    num_remember = int(remember_rate * len(loss_1_sorted))\n",
    "\n",
    "    pure_ratio_1 = torch.sum(noise_or_not[ind[ind_1_sorted[:num_remember]]].float()).item() / float(num_remember)\n",
    "    pure_ratio_2 = torch.sum(noise_or_not[ind[ind_2_sorted[:num_remember]]].float()).item() / float(num_remember)\n",
    "\n",
    "    ind_1_update = ind_1_sorted[:num_remember]\n",
    "    ind_2_update = ind_2_sorted[:num_remember]\n",
    "\n",
    "    # 교환\n",
    "    loss_1_update = F.cross_entropy(y_1[ind_2_update], t[ind_2_update])\n",
    "    loss_2_update = F.cross_entropy(y_2[ind_1_update], t[ind_1_update])\n",
    "\n",
    "    return (\n",
    "        torch.sum(loss_1_update) / num_remember,\n",
    "        torch.sum(loss_2_update) / num_remember,\n",
    "        pure_ratio_1,\n",
    "        pure_ratio_2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08fe0486-566b-40e4-95e9-2c343e574d6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T16:24:54.583740Z",
     "iopub.status.busy": "2024-11-24T16:24:54.582644Z",
     "iopub.status.idle": "2024-11-24T16:24:54.596379Z",
     "shell.execute_reply": "2024-11-24T16:24:54.595718Z",
     "shell.execute_reply.started": "2024-11-24T16:24:54.583716Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def call_bn(bn, x):\n",
    "    return bn(x)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_channel=3, n_outputs=10, dropout_rate=0.25, top_bn=False):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.top_bn = top_bn\n",
    "        super(CNN, self).__init__()\n",
    "        self.c1 = nn.Conv2d(input_channel, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.c2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.c3 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.c4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.c5 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.c6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.c7 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=0)\n",
    "        self.c8 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=0)\n",
    "        self.c9 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=0)\n",
    "        self.l_c1 = nn.Linear(128,n_outputs)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.bn6 = nn.BatchNorm2d(256)\n",
    "        self.bn7 = nn.BatchNorm2d(512)\n",
    "        self.bn8 = nn.BatchNorm2d(256)\n",
    "        self.bn9 = nn.BatchNorm2d(128)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        h = self.c1(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn1, h), negative_slope=0.01)\n",
    "        h = self.c2(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn2, h), negative_slope=0.01)\n",
    "        h = self.c3(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn3, h), negative_slope=0.01)\n",
    "        h = F.max_pool2d(h, kernel_size=2, stride=2)\n",
    "        h = F.dropout2d(h, p=self.dropout_rate)\n",
    "        \n",
    "        h = self.c4(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn4, h), negative_slope=0.01)\n",
    "        h = self.c5(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn5, h), negative_slope=0.01)\n",
    "        h = self.c6(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn6, h), negative_slope=0.01)\n",
    "        h = F.max_pool2d(h, kernel_size=2, stride=2)\n",
    "        h = F.dropout2d(h, p=self.dropout_rate)\n",
    "        \n",
    "        h = self.c7(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn7, h), negative_slope=0.01)\n",
    "        h = self.c8(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn8, h), negative_slope=0.01)\n",
    "        h = self.c9(h)\n",
    "        h = F.leaky_relu(call_bn(self.bn9, h), negative_slope=0.01)\n",
    "        h = F.avg_pool2d(h, kernel_size=h.data.shape[2])\n",
    "        \n",
    "        h = h.view(h.size(0), h.size(1))\n",
    "        logit = self.l_c1(h)\n",
    "        \n",
    "        if self.top_bn:\n",
    "            logit = call_bn(self.bn_c1, logit)\n",
    "            \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32d8ec98-cd0d-4e7f-ae95-1860f03bf104",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T16:24:54.599362Z",
     "iopub.status.busy": "2024-11-24T16:24:54.598300Z",
     "iopub.status.idle": "2024-11-24T16:24:54.606910Z",
     "shell.execute_reply": "2024-11-24T16:24:54.606309Z",
     "shell.execute_reply.started": "2024-11-24T16:24:54.599338Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 학습 및 평가 루프\n",
    "def train(train_loader, epoch, model1, optimizer1, model2, optimizer2, rate_schedule, noise_or_not):\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "    total_loss1, total_loss2 = 0, 0\n",
    "    for i, (images, labels, indexes) in enumerate(train_loader):\n",
    "        images = Variable(images).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "\n",
    "        logits1 = model1(images)\n",
    "        logits2 = model2(images)\n",
    "\n",
    "        loss_1, loss_2, _, _ = loss_coteaching(\n",
    "            logits1, logits2, labels, rate_schedule[epoch], indexes, noise_or_not\n",
    "        )\n",
    "\n",
    "        optimizer1.zero_grad()\n",
    "        loss_1.backward()\n",
    "        optimizer1.step()\n",
    "\n",
    "        optimizer2.zero_grad()\n",
    "        loss_2.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "        total_loss1 += loss_1.item()\n",
    "        total_loss2 += loss_2.item()\n",
    "\n",
    "    return total_loss1 / len(train_loader), total_loss2 / len(train_loader)\n",
    "\n",
    "# 평가 함수\n",
    "def evaluate(test_loader, model):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = Variable(images).cuda()\n",
    "            logits = model(images)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.cuda()).sum().item()\n",
    "    return 100 * correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f18243c-ef87-433f-b642-e31cd024f9d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T16:24:54.609928Z",
     "iopub.status.busy": "2024-11-24T16:24:54.608859Z",
     "iopub.status.idle": "2024-11-24T16:24:54.616679Z",
     "shell.execute_reply": "2024-11-24T16:24:54.616103Z",
     "shell.execute_reply.started": "2024-11-24T16:24:54.609903Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 학습 및 평가\n",
    "def run_training(\n",
    "    train_loader, test_loader, input_channel, num_classes, n_epochs, forget_rate, rate_schedule\n",
    "):\n",
    "    # 모델 및 옵티마이저 정의\n",
    "    model1 = CNN(input_channel=input_channel, n_outputs=num_classes)\n",
    "    model2 = CNN(input_channel=input_channel, n_outputs=num_classes)\n",
    "    model1.cuda()\n",
    "    model2.cuda()\n",
    "    optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.001)\n",
    "    optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
    "\n",
    "    # 노이즈 확인용 (옵션)\n",
    "    noise_or_not = torch.ones(len(train_loader.dataset), dtype=torch.bool)  # 예제 노이즈 여부\n",
    "\n",
    "    # 학습 루프\n",
    "    train_losses1, train_losses2 = [], []\n",
    "    test_accuracies1, test_accuracies2 = [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "        # 학습\n",
    "        train_loss1, train_loss2 = train(\n",
    "            train_loader,\n",
    "            epoch,\n",
    "            model1,\n",
    "            optimizer1,\n",
    "            model2,\n",
    "            optimizer2,\n",
    "            rate_schedule,\n",
    "            noise_or_not,\n",
    "        )\n",
    "        train_losses1.append(train_loss1)\n",
    "        train_losses2.append(train_loss2)\n",
    "\n",
    "        # 평가\n",
    "        test_acc1 = evaluate(test_loader, model1)\n",
    "        test_acc2 = evaluate(test_loader, model2)\n",
    "        test_accuracies1.append(test_acc1)\n",
    "        test_accuracies2.append(test_acc2)\n",
    "\n",
    "        print(\n",
    "            f\"Train Loss Model1: {train_loss1:.4f}, Model2: {train_loss2:.4f}, \"\n",
    "            f\"Test Accuracy Model1: {test_acc1:.2f}%, Model2: {test_acc2:.2f}%\"\n",
    "        )\n",
    "\n",
    "    return train_losses1, train_losses2, test_accuracies1, test_accuracies2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d3bacf2-5232-433e-b8b6-43ca035d58e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T16:24:54.620796Z",
     "iopub.status.busy": "2024-11-24T16:24:54.619728Z",
     "iopub.status.idle": "2024-11-24T16:24:55.691550Z",
     "shell.execute_reply": "2024-11-24T16:24:55.690928Z",
     "shell.execute_reply.started": "2024-11-24T16:24:54.620772Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_results(train_losses1, train_losses2, test_accuracies1, test_accuracies2):\n",
    "    epochs = len(train_losses1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(epochs), train_losses1, label=\"Model 1 Loss\")\n",
    "    plt.plot(range(epochs), train_losses2, label=\"Model 2 Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(epochs), test_accuracies1, label=\"Model 1 Accuracy\")\n",
    "    plt.plot(range(epochs), test_accuracies2, label=\"Model 2 Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(\"Test Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f02238-66c9-4d71-afa0-646e2dad91c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T16:24:55.694996Z",
     "iopub.status.busy": "2024-11-24T16:24:55.693961Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Train Loss Model1: 0.0105, Model2: 0.0105, Test Accuracy Model1: 58.30%, Model2: 61.12%\n",
      "Epoch 2/20\n",
      "Train Loss Model1: 0.0066, Model2: 0.0065, Test Accuracy Model1: 71.39%, Model2: 71.87%\n",
      "Epoch 3/20\n",
      "Train Loss Model1: 0.0047, Model2: 0.0048, Test Accuracy Model1: 75.76%, Model2: 75.66%\n",
      "Epoch 4/20\n",
      "Train Loss Model1: 0.0036, Model2: 0.0036, Test Accuracy Model1: 77.26%, Model2: 77.52%\n",
      "Epoch 5/20\n",
      "Train Loss Model1: 0.0028, Model2: 0.0028, Test Accuracy Model1: 77.35%, Model2: 79.33%\n",
      "Epoch 6/20\n",
      "Train Loss Model1: 0.0021, Model2: 0.0021, Test Accuracy Model1: 80.55%, Model2: 80.59%\n",
      "Epoch 7/20\n",
      "Train Loss Model1: 0.0017, Model2: 0.0017, Test Accuracy Model1: 81.17%, Model2: 81.65%\n",
      "Epoch 8/20\n",
      "Train Loss Model1: 0.0014, Model2: 0.0014, Test Accuracy Model1: 81.01%, Model2: 81.53%\n",
      "Epoch 9/20\n",
      "Train Loss Model1: 0.0011, Model2: 0.0011, Test Accuracy Model1: 81.06%, Model2: 82.40%\n",
      "Epoch 10/20\n",
      "Train Loss Model1: 0.0009, Model2: 0.0009, Test Accuracy Model1: 82.48%, Model2: 83.15%\n",
      "Epoch 11/20\n",
      "Train Loss Model1: 0.0009, Model2: 0.0009, Test Accuracy Model1: 82.07%, Model2: 82.81%\n",
      "Epoch 12/20\n",
      "Train Loss Model1: 0.0008, Model2: 0.0008, Test Accuracy Model1: 82.62%, Model2: 83.74%\n",
      "Epoch 13/20\n",
      "Train Loss Model1: 0.0007, Model2: 0.0007, Test Accuracy Model1: 82.97%, Model2: 82.60%\n",
      "Epoch 14/20\n",
      "Train Loss Model1: 0.0007, Model2: 0.0007, Test Accuracy Model1: 83.21%, Model2: 83.29%\n",
      "Epoch 15/20\n",
      "Train Loss Model1: 0.0006, Model2: 0.0006, Test Accuracy Model1: 84.71%, Model2: 83.38%\n",
      "Epoch 16/20\n",
      "Train Loss Model1: 0.0006, Model2: 0.0006, Test Accuracy Model1: 83.33%, Model2: 83.49%\n",
      "Epoch 17/20\n",
      "Train Loss Model1: 0.0006, Model2: 0.0006, Test Accuracy Model1: 82.83%, Model2: 84.16%\n",
      "Epoch 18/20\n",
      "Train Loss Model1: 0.0005, Model2: 0.0005, Test Accuracy Model1: 83.33%, Model2: 83.67%\n",
      "Epoch 19/20\n"
     ]
    }
   ],
   "source": [
    "# 설정\n",
    "n_epochs = 20  # 학습 epoch 수\n",
    "forget_rate = 0.2  # Forget Rate\n",
    "rate_schedule = np.ones(n_epochs) * forget_rate\n",
    "rate_schedule[:10] = np.linspace(0, forget_rate ** 1, 10)\n",
    "\n",
    "# 학습 실행\n",
    "train_losses1, train_losses2, test_accuracies1, test_accuracies2 = run_training(\n",
    "    train_loader, test_loader, input_channel, num_classes, n_epochs, forget_rate, rate_schedule\n",
    ")\n",
    "\n",
    "# 결과 시각화\n",
    "plot_results(train_losses1, train_losses2, test_accuracies1, test_accuracies2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a85c9b-7531-492f-8ca1-97d4b625bbef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.4.1 (py3.10)",
   "language": "python",
   "name": "pytorch-2.4.1-cuda12.4-py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
